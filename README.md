<!-- Gesture-Controlled Game Interface README -->

<p align="center">
  <img src="https://readme-typing-svg.herokuapp.com?font=Poppins&pause=1000&color=36BCF7&center=true&vCenter=true&width=900&lines=âœ‹ğŸ®+Gesture-Controlled+Game+Interface+using+Computer+Vision;Real-Time+Humanâ€“Computer+Interaction+System;OpenCV+%7C+MediaPipe+%7C+Python" alt="Typing Animation" />
</p>

<p align="center">
  <img src="https://img.shields.io/badge/Status-Completed-success?style=for-the-badge"/>
  <img src="https://img.shields.io/badge/Domain-Computer%20Vision%20%7C%20HCI-blue?style=for-the-badge"/>
  <img src="https://img.shields.io/badge/Python-3.10-yellow?style=for-the-badge"/>
  <img src="https://img.shields.io/badge/Made%20withâ¤ï¸by-Priyanka%20Asthana-ff69b4?style=for-the-badge"/>
</p>

---

## ğŸ¥ Project Demo & Video Walkthrough (Start Here)

<p align="center">
  <a href="PUT_YOUR_VIDEO_LINK_HERE">
    <img src="https://img.shields.io/badge/â–¶%20Watch%20Full%20Project%20Demo-red?style=for-the-badge&logo=youtube"/>
  </a>
</p>

<p align="center">
  <sub><i>Complete walkthrough explaining system design, gesture logic, and real-time interaction pipeline.</i></sub>
</p>

---

## ğŸš€ Overview

This project implements a **real-time gesture-controlled gaming interface** using **computer vision and humanâ€“computer interaction (HCI)** principles.

Hand gestures captured through a webcam are processed live and translated into **system-level keyboard inputs**, enabling **hands-free control of a game** running inside an Android emulator.

The goal was not just gesture detection, but building a **robust end-to-end interaction pipeline** that connects vision, decision logic, and OS-level control in real time.

---

## ğŸ§  System Pipeline (High-Level)

```text
Camera Input
   â†“
Hand Landmark Detection
   â†“
Gesture Classification
   â†“
Keyboard Event Mapping
   â†“
OS-Level Input Injection
   â†“
Game / Emulator Control
This pipeline is designed to be modular, extensible, and hardware-aware, making it suitable for experimentation with other applications beyond gaming.

âœ¨ Key Features
Feature	Description
âœ‹ Real-Time Hand Tracking	Tracks hand landmarks live using MediaPipe via CVZone
ğŸ¯ Gesture-Based Control	Interprets finger states (open palm, fist) into actions
âŒ¨ï¸ System-Level Input	Injects keyboard events using Python (pynput)
ğŸ® External App Integration	Controls a game running in an Android emulator
âš¡ Low Latency Pipeline	Designed for smooth real-time interaction
ğŸ§ª Experiment-Friendly	Easy to extend with new gestures or applications

ğŸ§° Tech Stack
<p align="center"> <img src="https://skillicons.dev/icons?i=python,opencv,git,vscode&theme=dark" /> </p>
Core Technologies

Python 3.10

OpenCV â€“ video capture and frame processing

MediaPipe â€“ hand landmark detection

CVZone â€“ abstraction over MediaPipe for gesture handling

pynput â€“ OS-level keyboard event simulation

ğŸ§­ System Architecture
mermaid
Copy code
graph TD
    A[Webcam Input] --> B[OpenCV Frame Processing]
    B --> C[MediaPipe Hand Landmarks]
    C --> D[Gesture Logic]
    D --> E[Keyboard Event Mapping]
    E --> F[OS-Level Input]
    F --> G[Game / Emulator]
ğŸš€ Getting Started
1ï¸âƒ£ Clone the Repository
bash
Copy code
git clone https://github.com/YOUR_USERNAME/gesture-controlled-game-interface.git
cd gesture-controlled-game-interface
2ï¸âƒ£ Create Virtual Environment
bash
Copy code
python -m venv venv
venv\Scripts\Activate   # Windows
3ï¸âƒ£ Install Dependencies
bash
Copy code
pip install -r requirements.txt
4ï¸âƒ£ Run the Project
bash
Copy code
python gesture_control_hill_climb.py
âš ï¸ Make sure the game/emulator window is in focus while running the script.

ğŸ§ª Gesture Mapping (Current)
Gesture	Action
âœŠ Fist	Move Left
ğŸ– Open Palm	Move Right
Other / No Hand	Neutral

ğŸ§© Challenges & Learnings
Managing real-time latency in vision pipelines

Handling gesture noise and lighting variability

Debugging Python version & dependency conflicts

Understanding limitations of deployability for hardware-dependent CV systems

Designing intuitive humanâ€“machine interaction loops

This project reinforced that debugging and integration are where real learning happens.

ğŸ”® Future Enhancements
ğŸ” Gesture smoothing & confidence thresholds

ğŸ¯ Separate acceleration / braking gestures

ğŸ“Š On-screen gesture feedback

ğŸ§  Gesture learning using ML classifiers

ğŸ•¶ Extension to AR/VR or assistive interfaces

ğŸ‘©â€ğŸ’» Author
Priyanka Asthana
ğŸ“ B.Tech (Hons) CSE | Minor in Robotics
ğŸ“ India
